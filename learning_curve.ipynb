{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from helper import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/original/pre-imputed/'\n",
    "C1W1nonet_df = pd.read_csv(datapath + 'C1W1_nonnetwork_preimputed.csv')\n",
    "C1W2nonet_df = pd.read_csv(datapath + 'C1W2_nonnetwork_preimputed.csv')\n",
    "mappings_df = pd.read_csv(datapath + 'mappings.csv')\n",
    "\n",
    "C1W1nonet_vars = list(C1W1nonet_df.columns)\n",
    "C1W2nonet_vars = list(C1W2nonet_df.columns)\n",
    "\n",
    "C1pred_df = pd.read_csv(datapath + 'C1_nonnetwork_pred.csv')\n",
    "C2pred_df = pd.read_csv(datapath + 'C2_nonnetwork_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See imputing notes from data_nonet_analysis.ipynb\n",
    "'''\n",
    "def impute_MARs(vars, df):\n",
    "\n",
    "    for v in vars:\n",
    "        col = df[v]\n",
    "        if v == 'TB2':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 5:\n",
    "                    df.at[idx, 'TB2_4_TEXT'] = -1\n",
    "                    df.at[idx, 'TB3'] = 1\n",
    "                    df.at[idx, 'TB4'] = 0\n",
    "        if v == 'TB3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB4'] = 0\n",
    "        if v == 'TB5':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB6'] = -1\n",
    "                    df.at[idx, 'TB7'] = 1\n",
    "                    df.at[idx, 'TB8'] = 0\n",
    "        if v == 'TB7':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB8'] = 0\n",
    "        if v == 'TB9':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB10'] = -1\n",
    "                    df.at[idx, 'TB11'] = 1\n",
    "                    df.at[idx, 'TB12'] = 0\n",
    "        if v == 'TB11':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB12'] = 0\n",
    "        if v == 'AL1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL1_4_TEXT'] = -1\n",
    "                    df.at[idx, 'AL2_1_TEXT'] = -1\n",
    "                    df.at[idx, 'AL3_1_TEXT'] = -1\n",
    "                    df.at[idx, 'AL4'] = 1\n",
    "                    df.at[idx, 'AL5'] = 0\n",
    "                    df.at[idx, 'AL6A'] = 0\n",
    "                    df.at[idx, 'AL6B'] = 0\n",
    "                elif i == 3:\n",
    "                    df.at[idx, 'AL1_4_TEXT'] = -2\n",
    "        if v == 'AL2':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL2_1_TEXT'] = -2\n",
    "        if v == 'AL3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL3_1_TEXT'] = -2\n",
    "        if v == 'AL5':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'AL6A'] = 0\n",
    "                    df.at[idx, 'AL6B'] = 0\n",
    "        if v == 'AL6A':\n",
    "            for idx, i in enumerate(col):\n",
    "                if pd.isnull(df.loc[idx, v]) and not pd.isnull(df.loc[idx, 'AL6B']):\n",
    "                    df.at[idx, v] = df.at[idx, 'AL6B']\n",
    "        if v == 'ID1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'ID2'] = -1\n",
    "                    for j in range(3,13):\n",
    "                        df.at[idx, f'ID{j}'] = 0\n",
    "                    for j in range(15,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ID3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(4,13):\n",
    "                        df.at[idx, f'ID{j}'] = 0\n",
    "                    for j in range(15,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ID17':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(18,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ND1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'ND2'] = -1\n",
    "        if v == 'OD1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD2'] = 0\n",
    "        if v == 'OD6':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    for j in range(7,12):\n",
    "                        df.at[idx, f'OD{j}'] = 0\n",
    "        if v == 'OD8':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD9'] = 0\n",
    "        if v == 'OD10':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD11'] = 0\n",
    "        if v == 'CJ3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(4,8):\n",
    "                        df.at[idx, f'CJ{j}'] = -1\n",
    "        if v == 'DM12':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i != 1:\n",
    "                    df.at[idx, 'DM13'] = -1\n",
    "\n",
    "    vars_mixed = ['DM1','TB2_4_TEXT','TB6','TB10','AL1_4_TEXT','AL2_1_TEXT','AL3_1_TEXT','ID2','ND2']\n",
    "    for v in vars_mixed:\n",
    "        col = df[v]\n",
    "        if v[-4:] == 'TEXT':  # e.g., modify TB2 column instead of TB2_4_TEXT\n",
    "            v = v.split('_')[0]\n",
    "        for idx, i in enumerate(col):\n",
    "            if 0 <= i <= 14:    df.at[idx, v] = 0  # children\n",
    "            elif 15 <= i <= 24: df.at[idx, v] = 1  # youth\n",
    "            elif 25 <= i <= 64: df.at[idx, v] = 2  # adult\n",
    "            elif i >= 65:       df.at[idx, v] = 3  # senior\n",
    "            elif i == -1:       df.at[idx, v] = -1  # never\n",
    "            elif i == -2 or (np.isnan(i) and df.at[idx, v] == 4):       df.at[idx, v] = np.nan  # don't know\n",
    "\n",
    "    v = 'SC1'  # numerical variable (# of years)\n",
    "    col = df[v]\n",
    "    for idx, i in enumerate(col):\n",
    "        if i < 0.5:    df.at[idx, v] = 0  # less than 6 months\n",
    "        elif 0.5 <= i < 1:  df.at[idx, v] = 1  # \n",
    "        elif 1 <= i < 2:    df.at[idx, v] = 2  #\n",
    "        elif 2 <= i < 5:    df.at[idx, v] = 3  #\n",
    "        elif 5 <= i < 10:    df.at[idx, v] = 4  #\n",
    "        elif i >= 10:       df.at[idx, v] = 5  # more than 10 years\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Feature Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and Impute\n",
    "\n",
    "After impute_MARs, all variables are categorical (including variables for ages and years)\n",
    "\n",
    "Note: 'Don't know' responses now will have value NaN instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c1w1 = impute_MARs(C1W1nonet_vars, C1W1nonet_df)\n",
    "discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "            'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df_c1w1.columns) if 'TEXT' in v]\n",
    "nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "indep_vars = list(df_c1w1.drop(columns=discarded_vars).columns)\n",
    "\n",
    "dep_var_full = []\n",
    "for a, b in zip(C1pred_df['ND1'], C1pred_df['Q68']):\n",
    "    if not np.isnan(a) and not np.isnan(b):\n",
    "        y = 0 if a <= b else 1\n",
    "        # y = a - b\n",
    "        dep_var_full.append(y)\n",
    "    else:   dep_var_full.append(np.nan)\n",
    "\n",
    "df = pd.concat([df_c1w1, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "X_nominal_df = X_df[nominal_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply label encoding for ordinal variables and one-hot encoding for nominal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols =[]\n",
    "for v in nominal_vars:\n",
    "    nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "\n",
    "Xenc_nominal_df = pd.concat(nominal_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables are categorical, so impute using the most frequent value of corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "X_imp = imp.fit_transform(Xenc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 152)\n",
      "Any nan left? False\n"
     ]
    }
   ],
   "source": [
    "print(X_imp.shape)\n",
    "print(f'Any nan left? {np.isnan(X_imp).any()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC (8): ['SC1', 'SC3', 'SC4', 'SC5', 'SC6', 'SC8', 'SC9', 'SC13']\n",
      "DM (28): ['DM1', 'DM9', 'DM14', 'DM15', 'DM17', 'DM23', 'DM8_1.0', 'DM8_2.0', 'DM8_3.0', 'DM8_4.0', 'DM8_5.0', 'DM8_6.0', 'DM10_1.0', 'DM10_2.0', 'DM10_5.0', 'DM10_6.0', 'DM10_7.0', 'DM10_8.0', 'DM10_9.0', 'DM12_1.0', 'DM12_2.0', 'DM12_3.0', 'DM12_5.0', 'DM12_6.0', 'DM13_-1.0', 'DM13_1.0', 'DM13_4.0', 'DM13_5.0']\n",
      "TB (12): ['TB1', 'TB2', 'TB3', 'TB4', 'TB5', 'TB6', 'TB7', 'TB8', 'TB9', 'TB10', 'TB11', 'TB12']\n",
      "AL (6): ['AL1', 'AL2', 'AL3', 'AL4', 'AL5', 'AL6A']\n",
      "ID (18): ['ID1', 'ID2', 'ID3', 'ID4', 'ID5', 'ID6', 'ID7', 'ID8', 'ID9', 'ID10', 'ID11', 'ID12', 'ID15', 'ID16', 'ID17', 'ID18', 'ID19', 'ID20']\n",
      "ND (12): ['ND1', 'ND2', 'ND3', 'ND4', 'ND5', 'ND6', 'ND7', 'ND8', 'ND9', 'ND10', 'ND11', 'ND12']\n",
      "DA (14): ['DA1_1', 'DA1_2', 'DA1_3', 'DA1_4', 'DA1_5', 'DA1_6', 'DA1_7', 'DA2_1', 'DA2_2', 'DA2_3', 'DA2_4', 'DA2_5', 'DA2_6', 'DA2_7']\n",
      "OD (8): ['OD1', 'OD2', 'OD6', 'OD7', 'OD8', 'OD9', 'OD10', 'OD11']\n",
      "TX (14): ['TX1_1', 'TX1_2', 'TX1_3', 'TX1_4', 'TX1_5', 'TX1_6', 'TX1_7', 'TX2_1', 'TX2_2', 'TX2_3', 'TX2_4', 'TX2_5', 'TX2_6', 'TX2_7']\n",
      "AC (25): ['AC1A', 'AC1B', 'AC2A', 'AC2B', 'AC3A', 'AC3B', 'AC3C', 'AC3D', 'AC4A', 'AC4B', 'AC5A', 'AC5B', 'AC6', 'AC7A', 'AC7B', 'AC7C', 'AC8A', 'AC8B', 'AC9A', 'AC9B', 'AC10', 'AC11', 'AC12', 'AC13', 'AC14']\n",
      "CJ (7): ['CJ1', 'CJ2', 'CJ3', 'CJ4', 'CJ5', 'CJ6', 'CJ7']\n"
     ]
    }
   ],
   "source": [
    "f_dict = {}\n",
    "fgroups = ['SC', 'DM', 'TB', 'AL', 'ID', 'ND', 'DA', 'OD', 'TX', 'AC', 'CJ']  # feature groups\n",
    "for g in fgroups:\n",
    "    features = [c for c in Xenc_df if c.startswith(g)]\n",
    "    f_indices = [Xenc_df.columns.get_loc(c) for c in Xenc_df if c.startswith(g)]  # column indices of the group's features\n",
    "    f_dict[g] = f_indices\n",
    "    print(f'{g} ({len(features)}): {features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: access feature group 'AL' directly from X_imp (2D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_imp[:,f_dict['AL']].shape)\n",
    "X_imp[:,f_dict['AL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: access list of feature groups ['AL', 'ID'] directly from X_imp (2D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., 1., 3., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 2., 2., 1., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [2., 2., 3., 1., 6., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 2., 2., 1., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [2., 2., 2., 1., 4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsubsets = ['AL', 'ID']\n",
    "def get_f_indices(fsubsets):  # concatenate multiple feature groups (lists) into one big list of features\n",
    "    return [f for fgroup in [f_dict[s] for s in fsubsets] for f in fgroup]\n",
    "\n",
    "X_imp[:,get_f_indices(fsubsets)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsubs_1, fsubs_2, fsubs_3, fsubs_4 = ['SC'], ['DM'], ['TB','AL','ID','ND','DA','OD'], ['TX','AC','CJ']\n",
    "fsubs_5 = fsubs_1 + fsubs_2\n",
    "fsubs_6 = fsubs_1 + fsubs_2 + fsubs_3\n",
    "fsubs_7 = fsubs_1 + fsubs_2 + fsubs_4\n",
    "fsubs_8 = fsubs_1 + fsubs_2 + fsubs_3 + fsubs_4\n",
    "fsubs_nonnet = {'g1': fsubs_1, 'g2': fsubs_2, 'g3': fsubs_3, 'g4': fsubs_4, 'g5': fsubs_5, 'g6': fsubs_6, 'g7': fsubs_7, 'g8': fsubs_8}\n",
    "# X_imp[:,get_f_indices(fsubsets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(dep_var_full)\n",
    "y = y[~np.isnan(y)]  # C1, marijuana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    scoring=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    score=None, baseline=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    scoring : str or callable, default=None\n",
    "        A str (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    if score is not None:\n",
    "        axes.plot([], [], ' ', label=f\"CV acc/baseline acc: {round(score, 3)}/{round(baseline, 3)}\")\n",
    "\n",
    "    axes.legend(loc=\"lower right\")\n",
    "\n",
    "    # # Plot n_samples vs fit_times\n",
    "    # axes[1].grid()\n",
    "    # axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    # axes[1].fill_between(\n",
    "    #     train_sizes,\n",
    "    #     fit_times_mean - fit_times_std,\n",
    "    #     fit_times_mean + fit_times_std,\n",
    "    #     alpha=0.1,\n",
    "    # )\n",
    "    # axes[1].set_xlabel(\"Training examples\")\n",
    "    # axes[1].set_ylabel(\"fit_times\")\n",
    "    # axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # # Plot fit_time vs score\n",
    "    # fit_time_argsort = fit_times_mean.argsort()\n",
    "    # fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    # test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    # test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    # axes[2].grid()\n",
    "    # axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    # axes[2].fill_between(\n",
    "    #     fit_time_sorted,\n",
    "    #     test_scores_mean_sorted - test_scores_std_sorted,\n",
    "    #     test_scores_mean_sorted + test_scores_std_sorted,\n",
    "    #     alpha=0.1,\n",
    "    # )\n",
    "    # axes[2].set_xlabel(\"fit_times\")\n",
    "    # axes[2].set_ylabel(\"Score\")\n",
    "    # axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the best params found from GridSearchCV into the model's learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def log_regression_tuned(X, y):\n",
    "    # param_grid = dict(penalty=['l1', 'l2'], C=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "    # clf = GridSearchCV(estimator=LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    #                     param_grid=param_grid,\n",
    "    #                     scoring='accuracy',\n",
    "    #                     cv=LeaveOneOut())\n",
    "    # clf.fit(standard_scale(X), y)\n",
    "\n",
    "    # return clf.best_estimator_, clf.best_score_\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    loocv_scores = cross_val_score(clf, standard_scale(X), y, cv=LeaveOneOut())\n",
    "\n",
    "    return clf, np.mean(loocv_scores)\n",
    "\n",
    "def pruned_decision_trees_tuned(X, y):\n",
    "    # param_grid = dict(max_depth=range(2,6), min_samples_split=[5, 10, 15], min_samples_leaf=range(1,6))\n",
    "    # clf = GridSearchCV(estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "    #                     param_grid=param_grid,\n",
    "    #                     scoring='accuracy',\n",
    "    #                     cv=StratifiedKFold())\n",
    "    clf = DecisionTreeClassifier()\n",
    "    loocv_scores = cross_val_score(clf, X, y, cv=LeaveOneOut())\n",
    "\n",
    "    return clf, np.mean(loocv_scores)\n",
    "\n",
    "def svm_tuned(X, y):\n",
    "    # param_grid = dict(kernel=['linear','poly','rbf','sigmoid'], C=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "    # clf = GridSearchCV(estimator=SVC(class_weight='balanced'),\n",
    "    #                     param_grid=param_grid,\n",
    "    #                     scoring='accuracy',\n",
    "    #                     cv=StratifiedKFold())\n",
    "    clf = SVC()\n",
    "    loocv_scores = cross_val_score(clf, standard_scale(X), y, cv=LeaveOneOut())\n",
    "\n",
    "    return clf, np.mean(loocv_scores)\n",
    "\n",
    "\n",
    "def plot_LC_tuned(X, y, cohort, drug, fgroup, savefile=None, baseline=1, n_estimators=3):\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_estimators, figsize=(15, 5))\n",
    "\n",
    "    # Cross validation with 50 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n",
    "\n",
    "    title = \"Learning Curves (logistic regression)\"\n",
    "    estimator, lg_score = log_regression_tuned(X, y)\n",
    "    plot_learning_curve(\n",
    "        estimator, title, standard_scale(X), y, axes=axes[0], ylim=(0.1, 1.01), cv=cv, n_jobs=4, scoring=\"accuracy\", score=lg_score, baseline=baseline\n",
    "    )\n",
    "\n",
    "    title = \"Learning Curves (decision tree)\"\n",
    "    estimator, dt_score = pruned_decision_trees_tuned(X, y)\n",
    "    plot_learning_curve(\n",
    "        estimator, title, X, y, axes=axes[1], ylim=(0.1, 1.01), cv=cv, n_jobs=4, scoring=\"accuracy\", score=dt_score, baseline=baseline\n",
    "    )\n",
    "\n",
    "    title = \"Learning Curves (SVM)\"\n",
    "    estimator, svm_score = svm_tuned(X, y)\n",
    "    plot_learning_curve(\n",
    "        estimator, title, standard_scale(X), y, axes=axes[2], ylim=(0.1, 1.01), cv=cv, n_jobs=4, scoring=\"accuracy\", score=svm_score, baseline=baseline\n",
    "    )\n",
    "\n",
    "    suptitle = f'Cohort {cohort}: {drug} use, features {fgroup}'\n",
    "    if len(suptitle) > 120:     suptitle = '-\\n'.join(suptitle[j:j+120] for j in range(0,len(suptitle), 120))  # break line if title too long\n",
    "    fig.suptitle(suptitle)\n",
    "    fig.tight_layout()\n",
    "    if savefile is None:\n",
    "        plt.savefig(f'plots/analysis/learning_curve/C{cohort}-{drug}/C{cohort}-{drug}-{fgroup}_learningCurve.pdf', facecolor='white')\n",
    "    else:\n",
    "        plt.savefig(savefile, facecolor='white')\n",
    "\n",
    "    return lg_score, dt_score, svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = 'marijuana'\n",
    "\n",
    "for gname, fsubs in fsubs_nonnet.items():\n",
    "    X = X_imp[:,get_f_indices(fsubs)]\n",
    "    plot_LC_tuned(X, y, drug, gname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = 'data/original/pre-imputed/'\n",
    "\n",
    "C1W1nonet_df = pd.read_csv(datapath + 'C1W1_nonnetwork_preimputed.csv')\n",
    "C1pred_df = pd.read_csv(datapath + 'C1_nonnetwork_pred.csv')\n",
    "C1W1nonet_vars = list(C1W1nonet_df.columns)\n",
    "\n",
    "C2W1nonet_df = pd.read_csv(datapath + 'C2W1_nonnetwork_preimputed.csv')\n",
    "C2pred_df = pd.read_csv(datapath + 'C2_nonnetwork_pred.csv')\n",
    "C2W1nonet_vars = list(C2W1nonet_df.columns)\n",
    "\n",
    "C1W1nonet_vars == C2W1nonet_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_indices(fsubsets):  # concatenate multiple feature groups (lists) into one big list of features\n",
    "    return [f for fgroup in [f_dict[s] for s in fsubsets] for f in fgroup]\n",
    "\n",
    "fsubs_1, fsubs_2, fsubs_3, fsubs_4 = ['SC'], ['DM'], ['TB','AL','ID','ND','DA','OD'], ['TX','AC','CJ']\n",
    "fsubs_5 = fsubs_1 + fsubs_2\n",
    "fsubs_6 = fsubs_1 + fsubs_2 + fsubs_3\n",
    "fsubs_7 = fsubs_1 + fsubs_2 + fsubs_4\n",
    "fsubs_8 = fsubs_1 + fsubs_2 + fsubs_3 + fsubs_4\n",
    "fsubs_nonnet = {'g1': fsubs_1, 'g2': fsubs_2, 'g3': fsubs_3, 'g4': fsubs_4, 'g5': fsubs_5, 'g6': fsubs_6, 'g7': fsubs_7, 'g8': fsubs_8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohorts 1 and 2 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in range(1,3):\n",
    "    for drug in ['marijuana', 'meth']:\n",
    "\n",
    "        if cohort == 1:\n",
    "            nonet_vars, nonet_df = C1W1nonet_vars, C1W1nonet_df\n",
    "            pred_df = C1pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        elif cohort == 2:\n",
    "            nonet_vars, nonet_df = C2W1nonet_vars, C2W1nonet_df\n",
    "            pred_df = C2pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['W2_ND1']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['W2_ND7'])\n",
    "\n",
    "        df = impute_MARs(nonet_vars, nonet_df)\n",
    "        discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                    'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df.columns) if 'TEXT' in v]\n",
    "        nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "        indep_vars = list(df.drop(columns=discarded_vars).columns)\n",
    "        \n",
    "        dep_var_full = []\n",
    "        for a, b in pred_var:\n",
    "            if not np.isnan(a) and not np.isnan(b):\n",
    "                y = 0 if a <= b else 1\n",
    "                # y = a - b\n",
    "                dep_var_full.append(y)\n",
    "            else:   dep_var_full.append(np.nan)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "        X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "        X_nominal_df = X_df[nominal_vars]\n",
    "\n",
    "        # Encode\n",
    "        Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "        Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "        nominal_cols =[]\n",
    "        for v in nominal_vars:\n",
    "            nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "        Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "        Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "        # Group features\n",
    "        f_dict = {}\n",
    "        fgroups = ['SC', 'DM', 'TB', 'AL', 'ID', 'ND', 'DA', 'OD', 'TX', 'AC', 'CJ']  # feature groups\n",
    "        for g in fgroups:\n",
    "            features = [c for c in Xenc_df if c.startswith(g)]\n",
    "            f_indices = [Xenc_df.columns.get_loc(c) for c in Xenc_df if c.startswith(g)]  # column indices of the group's features\n",
    "            f_dict[g] = f_indices\n",
    "\n",
    "        # Impute\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "        # Learning curve\n",
    "        y = np.array(dep_var_full)\n",
    "        y = y[~np.isnan(y)]\n",
    "        baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "        for gname, fsubs in fsubs_nonnet.items():\n",
    "            X = X_imp[:,get_f_indices(fsubs)]\n",
    "            plot_LC_tuned(X, y, cohort, drug, gname, baseline=baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohorts 1 and 2 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drug in ['marijuana', 'meth']:\n",
    "\n",
    "    nonet_df = pd.concat([C1W1nonet_df, C2W1nonet_df], ignore_index=True)\n",
    "    nonet_vars = C1W1nonet_vars  # same set of columns for both cohorts\n",
    "\n",
    "    colname_map = {}\n",
    "    C2pred_keys = list(C2pred_df.columns)\n",
    "    for i, c in enumerate(list(C1pred_df.columns)):  # map column names of C2pred_df to C1pred_df (since C1W2 has different varnames)\n",
    "        colname_map[C2pred_keys[i]] = c\n",
    "\n",
    "    df_pred = pd.concat([C1pred_df, C2pred_df.rename(columns=colname_map)], ignore_index=True)\n",
    "    pred_var = zip(df_pred['ND1'], df_pred['Q68']) if drug == 'marijuana' else zip(df_pred['ND7'], df_pred['Q75'])\n",
    "\n",
    "    df = impute_MARs(nonet_vars, nonet_df)\n",
    "    discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df.columns) if 'TEXT' in v]\n",
    "    nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "    indep_vars = list(df.drop(columns=discarded_vars).columns)\n",
    "    \n",
    "    dep_var_full = []\n",
    "    for a, b in pred_var:\n",
    "        if not np.isnan(a) and not np.isnan(b):\n",
    "            y = 0 if a <= b else 1\n",
    "            # y = a - b\n",
    "            dep_var_full.append(y)\n",
    "        else:   dep_var_full.append(np.nan)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "    X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "    X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "    X_nominal_df = X_df[nominal_vars]\n",
    "\n",
    "    # Encode\n",
    "    Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "    Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "    nominal_cols =[]\n",
    "    for v in nominal_vars:\n",
    "        nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "    Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "    Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "    # Group features\n",
    "    f_dict = {}\n",
    "    fgroups = ['SC', 'DM', 'TB', 'AL', 'ID', 'ND', 'DA', 'OD', 'TX', 'AC', 'CJ']  # feature groups\n",
    "    for g in fgroups:\n",
    "        features = [c for c in Xenc_df if c.startswith(g)]\n",
    "        f_indices = [Xenc_df.columns.get_loc(c) for c in Xenc_df if c.startswith(g)]  # column indices of the group's features\n",
    "        f_dict[g] = f_indices\n",
    "\n",
    "    # Impute\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "    # Learning curve\n",
    "    y = np.array(dep_var_full)\n",
    "    y = y[~np.isnan(y)]\n",
    "    baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "    for gname, fsubs in fsubs_nonnet.items():\n",
    "        X = X_imp[:,get_f_indices(fsubs)]\n",
    "        savefile = f'plots/analysis/learning_curve/C1+2-{drug}/C1+2-{drug}-{gname}_learningCurve.pdf'\n",
    "        lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort='1+2', drug=drug, fgroup=gname, savefile=savefile, baseline=baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Feature Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively add features into X (sorted by decreasing correlation index) then plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "scores = {}\n",
    "N_top = 30  # max number of features (with top correlation indices)\n",
    "for cohort in range(1,3):\n",
    "    for drug in ['marijuana', 'meth']:\n",
    "\n",
    "        if cohort == 1:\n",
    "            nonet_vars, nonet_df = C1W1nonet_vars, C1W1nonet_df\n",
    "            pred_df = C1pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        elif cohort == 2:\n",
    "            nonet_vars, nonet_df = C2W1nonet_vars, C2W1nonet_df\n",
    "            pred_df = C2pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['W2_ND1']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['W2_ND7'])\n",
    "\n",
    "        df = impute_MARs(nonet_vars, nonet_df)\n",
    "        discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                    'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df.columns) if 'TEXT' in v]\n",
    "        nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "        indep_vars = list(df.drop(columns=discarded_vars).columns)\n",
    "        \n",
    "        dep_var_full = []\n",
    "        for a, b in pred_var:\n",
    "            if not np.isnan(a) and not np.isnan(b):\n",
    "                y = 0 if a <= b else 1\n",
    "                # y = a - b\n",
    "                dep_var_full.append(y)\n",
    "            else:   dep_var_full.append(np.nan)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "        X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "        X_nominal_df = X_df[nominal_vars]\n",
    "\n",
    "        # Encode\n",
    "        Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "        Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "        nominal_cols =[]\n",
    "        for v in nominal_vars:\n",
    "            nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "        Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "        Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "        # Impute\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "        # Load corrs_dict:\n",
    "        with open(f'saved-vars/C{cohort}-{drug}-nonnet_CramersV.pkl', 'rb') as file:\n",
    "            corrs_dict = pickle.load(file)\n",
    "        sorted_corrs_dict = dict(sorted(corrs_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        sorted_corrs_list = [v for v in sorted_corrs_dict.keys()]\n",
    "\n",
    "        # Learning curve\n",
    "        y = np.array(dep_var_full)\n",
    "        y = y[~np.isnan(y)]\n",
    "        baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "        train_vars = []\n",
    "        for i in range(N_top):\n",
    "            var = sorted_corrs_list[i]\n",
    "            if var in nominal_vars:\n",
    "                train_vars.extend([v for v in Xenc_df if v.startswith(var)])\n",
    "            else:   train_vars.append(var)\n",
    "\n",
    "            col_indices = sorted([Xenc_df.columns.get_loc(v) for v in train_vars])\n",
    "            X = X_imp[:,col_indices]\n",
    "            savefile = f'plots/analysis/learning_curve/C{cohort}-{drug}/C{cohort}-{drug}-top{i+1}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, train_vars, savefile=savefile, baseline=baseline)\n",
    "            scores[f'C{cohort}-{drug}-{i+1}'] = [lg_score, dt_score, svm_score]\n",
    "\n",
    "with open(f'saved-vars/scores.pkl', 'wb') as file:\n",
    "    pickle.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "N_top = 30  # max number of features (with top correlation indices)\n",
    "for drug in ['marijuana', 'meth']:\n",
    "\n",
    "    nonet_df = pd.concat([C1W1nonet_df, C2W1nonet_df], ignore_index=True)\n",
    "    nonet_vars = C1W1nonet_vars  # same set of columns for both cohorts\n",
    "\n",
    "    colname_map = {}\n",
    "    C2pred_keys = list(C2pred_df.columns)\n",
    "    for i, c in enumerate(list(C1pred_df.columns)):  # map column names of C2pred_df to C1pred_df (since C1W2 has different varnames)\n",
    "        colname_map[C2pred_keys[i]] = c\n",
    "\n",
    "    df_pred = pd.concat([C1pred_df, C2pred_df.rename(columns=colname_map)], ignore_index=True)\n",
    "    pred_var = zip(df_pred['ND1'], df_pred['Q68']) if drug == 'marijuana' else zip(df_pred['ND7'], df_pred['Q75'])\n",
    "\n",
    "    df = impute_MARs(nonet_vars, nonet_df)\n",
    "    discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df.columns) if 'TEXT' in v]\n",
    "    nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "    indep_vars = list(df.drop(columns=discarded_vars).columns)\n",
    "    \n",
    "    dep_var_full = []\n",
    "    for a, b in pred_var:\n",
    "        if not np.isnan(a) and not np.isnan(b):\n",
    "            y = 0 if a <= b else 1\n",
    "            # y = a - b\n",
    "            dep_var_full.append(y)\n",
    "        else:   dep_var_full.append(np.nan)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "    X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "    X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "    X_nominal_df = X_df[nominal_vars]\n",
    "\n",
    "    # Encode\n",
    "    Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "    Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "    nominal_cols =[]\n",
    "    for v in nominal_vars:\n",
    "        nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "    Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "    Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "    # Load corrs_dict:\n",
    "    with open(f'saved-vars/C12-{drug}-nonnet_CramersV.pkl', 'rb') as file:\n",
    "        corrs_dict = pickle.load(file)\n",
    "    sorted_corrs_dict = dict(sorted(corrs_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_corrs_list = [v for v in sorted_corrs_dict.keys()]\n",
    "\n",
    "    # Learning curve\n",
    "    y = np.array(dep_var_full)\n",
    "    y = y[~np.isnan(y)]\n",
    "    baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "    train_vars = []\n",
    "    for i in range(N_top):\n",
    "        var = sorted_corrs_list[i]\n",
    "        if var in nominal_vars:\n",
    "            train_vars.extend([v for v in Xenc_df if v.startswith(var)])\n",
    "        else:   train_vars.append(var)\n",
    "\n",
    "        col_indices = sorted([Xenc_df.columns.get_loc(v) for v in train_vars])\n",
    "        X = X_imp[:,col_indices]\n",
    "        savefile = f'plots/analysis/learning_curve/C1+2-{drug}/C1+2-{drug}-top{i+1}_learningCurve.pdf'\n",
    "        lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort='1+2', drug=drug, fgroup=train_vars, savefile=savefile, baseline=baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static + Dynamic Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-network variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "scores = {}\n",
    "scores_dict = {}\n",
    "N_top = 30  # max number of features (with top correlation indices)\n",
    "for cohort in [1, 2, '1+2']:\n",
    "    for drug in ['marijuana', 'meth']:\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-fgroup'] = []\n",
    "        scores_dict[f'{cohort}-{drug}-LG'], scores_dict[f'{cohort}-{drug}-DT'], scores_dict[f'{cohort}-{drug}-SVM'] = [], [], []\n",
    "\n",
    "        if cohort == 1:\n",
    "            nonet_vars, nonet_df = C1W1nonet_vars, C1W1nonet_df\n",
    "            pred_df = C1pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        elif cohort == 2:\n",
    "            nonet_vars, nonet_df = C2W1nonet_vars, C2W1nonet_df\n",
    "            pred_df = C2pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['W2_ND1']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['W2_ND7'])\n",
    "\n",
    "        elif cohort == '1+2':\n",
    "            nonet_df = pd.concat([C1W1nonet_df, C2W1nonet_df], ignore_index=True)\n",
    "            nonet_vars = C1W1nonet_vars  # same set of columns for both cohorts\n",
    "\n",
    "            colname_map = {}\n",
    "            C2pred_keys = list(C2pred_df.columns)\n",
    "            for i, c in enumerate(list(C1pred_df.columns)):  # map column names of C2pred_df to C1pred_df (since C1W2 has different varnames)\n",
    "                colname_map[C2pred_keys[i]] = c\n",
    "\n",
    "            pred_df = pd.concat([C1pred_df, C2pred_df.rename(columns=colname_map)], ignore_index=True)\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "\n",
    "        df = impute_MARs(nonet_vars, nonet_df)\n",
    "        discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                    'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(df.columns) if 'TEXT' in v]\n",
    "        nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "        indep_vars = list(df.drop(columns=discarded_vars).columns)\n",
    "        \n",
    "        dep_var_full = []\n",
    "        for a, b in pred_var:\n",
    "            if not np.isnan(a) and not np.isnan(b):\n",
    "                y = 0 if a <= b else 1\n",
    "                # y = a - b\n",
    "                dep_var_full.append(y)\n",
    "            else:   dep_var_full.append(np.nan)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        X_df = df[df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "        X_ordinal_df = X_df.drop(nominal_vars, axis=1)\n",
    "        X_nominal_df = X_df[nominal_vars]\n",
    "\n",
    "        # Encode\n",
    "        Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "        Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "        nominal_cols =[]\n",
    "        for v in nominal_vars:\n",
    "            nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "        Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "        Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "        # Group features\n",
    "        f_dict = {}\n",
    "        fgroups = ['SC', 'DM', 'TB', 'AL', 'ID', 'ND', 'DA', 'OD', 'TX', 'AC', 'CJ']  # feature groups\n",
    "        for g in fgroups:\n",
    "            features = [c for c in Xenc_df if c.startswith(g)]\n",
    "            f_indices = [Xenc_df.columns.get_loc(c) for c in Xenc_df if c.startswith(g)]  # column indices of the group's features\n",
    "            f_dict[g] = f_indices\n",
    "\n",
    "        # Impute\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "        # Load corrs_dict:\n",
    "        with open(f\"saved-vars/C{''.join(str(cohort).split('+'))}-{drug}-nonnet_CramersV.pkl\", 'rb') as file:\n",
    "            corrs_dict = pickle.load(file)\n",
    "        corrs_dict = {k: v for k, v in corrs_dict.items() if not np.isnan(v)}  # sort dict by value won't work properly if there is a key w/ nan value\n",
    "        sorted_corrs_dict = dict(sorted(corrs_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        sorted_corrs_list = [v for v in sorted_corrs_dict.keys()]\n",
    "\n",
    "        # Learning curve\n",
    "        y = np.array(dep_var_full)\n",
    "        y = y[~np.isnan(y)]\n",
    "        baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "        # Static grouping\n",
    "        for gname, fsubs in fsubs_nonnet.items():\n",
    "            X = X_imp[:, [f for fgroup in [f_dict[s] for s in fsubs] for f in fgroup]]\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, gname, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(gname)\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        # Dynamic grouping\n",
    "        train_vars = []\n",
    "        for i in range(N_top):\n",
    "            var = sorted_corrs_list[i]\n",
    "            if var in nominal_vars:\n",
    "                train_vars.extend([v for v in Xenc_df if v.startswith(var)])\n",
    "            else:   train_vars.append(var)\n",
    "\n",
    "            col_indices = sorted([Xenc_df.columns.get_loc(v) for v in train_vars])\n",
    "            X = X_imp[:,col_indices]\n",
    "            savefile = f'plots/analysis/learning_curve/C{cohort}-{drug}/C{cohort}-{drug}-top{i+1}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, train_vars, savefile=savefile, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(sorted_corrs_list[:i+1])\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-baseline'] = [baseline] * (len(fsubs_nonnet)+N_top)\n",
    "\n",
    "pd.DataFrame.from_dict(scores_dict).to_csv('results/scores_nonnetwork.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsubs_net = {'g9': ['NSX'], 'g10': ['NDX'], 'g11': ['NSX','NDX']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "scores = {}\n",
    "scores_dict = {}\n",
    "N_top = 30  # max number of features (with top correlation indices)\n",
    "for cohort in [1, 2, '1+2']:\n",
    "    for drug in ['marijuana', 'meth']:\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-fgroup'] = []\n",
    "        scores_dict[f'{cohort}-{drug}-LG'], scores_dict[f'{cohort}-{drug}-DT'], scores_dict[f'{cohort}-{drug}-SVM'] = [], [], []\n",
    "\n",
    "        # csv generated from data_net_analysis.ipynb (Cramer's V section)\n",
    "        df = pd.read_csv(f\"saved-vars/C{''.join(str(cohort).split('+'))}_network-processed.csv\")\n",
    "        if cohort == 1:\n",
    "            pred_df = C1pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        elif cohort == 2:\n",
    "            pred_df = C2pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['W2_ND1']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['W2_ND7'])\n",
    "\n",
    "        elif cohort == '1+2':\n",
    "            colname_map = {}\n",
    "            C2pred_keys = list(C2pred_df.columns)\n",
    "            for i, c in enumerate(list(C1pred_df.columns)):  # map column names of C2pred_df to C1pred_df (since C1W2 has different varnames)\n",
    "                colname_map[C2pred_keys[i]] = c\n",
    "\n",
    "            pred_df = pd.concat([C1pred_df, C2pred_df.rename(columns=colname_map)], ignore_index=True)\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        \n",
    "        dep_var_full = []\n",
    "        for a, b in pred_var:\n",
    "            if not np.isnan(a) and not np.isnan(b):\n",
    "                y = 0 if a <= b else 1\n",
    "                # y = a - b\n",
    "                dep_var_full.append(y)\n",
    "            else:   dep_var_full.append(np.nan)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        X_df = df[df['pred'].notna()].drop('pred', axis=1)\n",
    "\n",
    "        # Group features\n",
    "        f_dict = {}\n",
    "        fgroups = ['NSX','NDX']  # feature groups\n",
    "        for g in fgroups:\n",
    "            features = [c for c in X_df if c.startswith(g) or g[:2] in c]\n",
    "            f_indices = [X_df.columns.get_loc(c) for c in X_df if c.startswith(g)]  # column indices of the group's features\n",
    "            f_dict[g] = f_indices\n",
    "\n",
    "        X_np = X_df.to_numpy()\n",
    "\n",
    "        # Load corrs_dict:\n",
    "        with open(f\"saved-vars/C{''.join(str(cohort).split('+'))}-{drug}-net_CramersV.pkl\", 'rb') as file:\n",
    "            corrs_dict = pickle.load(file)\n",
    "        corrs_dict = {k: v for k, v in corrs_dict.items() if not np.isnan(v)}  # sort dict by value won't work properly if there is a key w/ nan value\n",
    "        sorted_corrs_dict = dict(sorted(corrs_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        sorted_corrs_list = [v for v in sorted_corrs_dict.keys()]\n",
    "\n",
    "        # Learning curve\n",
    "        y = np.array(dep_var_full)\n",
    "        y = y[~np.isnan(y)]\n",
    "        baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "        # Static grouping\n",
    "        for gname, fsubs in fsubs_net.items():\n",
    "            X = X_np[:, [f for fgroup in [f_dict[s] for s in fsubs] for f in fgroup]]\n",
    "            savefile = f'plots/analysis/learning_curve/net/C{cohort}-{drug}/C{cohort}-{drug}-{gname}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, gname, savefile=savefile, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(gname)\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        # Dynamic grouping\n",
    "        train_vars = []\n",
    "        for i in range(N_top):\n",
    "            var = sorted_corrs_list[i]\n",
    "            train_vars.append(var)\n",
    "\n",
    "            col_indices = sorted([X_df.columns.get_loc(v) for v in train_vars])\n",
    "            X = X_np[:,col_indices]\n",
    "            savefile = f'plots/analysis/learning_curve/net/C{cohort}-{drug}/C{cohort}-{drug}-top{i+1}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, train_vars, savefile=savefile, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(sorted_corrs_list[:i+1])\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-baseline'] = [baseline] * (len(fsubs_net)+N_top)\n",
    "\n",
    "pd.DataFrame.from_dict(scores_dict).to_csv('results/scores_network.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-network + network variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsubs_netnonnet = { 'g12': fsubs_nonnet['g3']+fsubs_net['g10'],\n",
    "                    'g13': fsubs_nonnet['g6']+fsubs_net['g10'],\n",
    "                    'g14': fsubs_nonnet['g8']+fsubs_net['g11']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "scores = {}\n",
    "scores_dict = {}\n",
    "N_top = 30  # max number of features (with top correlation indices)\n",
    "for cohort in [1, 2, '1+2']:\n",
    "    for drug in ['marijuana', 'meth']:\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-fgroup'] = []\n",
    "        scores_dict[f'{cohort}-{drug}-LG'], scores_dict[f'{cohort}-{drug}-DT'], scores_dict[f'{cohort}-{drug}-SVM'] = [], [], []\n",
    "\n",
    "        net_df = pd.read_csv(f\"saved-vars/C{''.join(str(cohort).split('+'))}_network-processed.csv\")\n",
    "        if cohort == 1:\n",
    "            nonet_vars, nonet_df = C1W1nonet_vars, C1W1nonet_df\n",
    "            pred_df = C1pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "        elif cohort == 2:\n",
    "            nonet_vars, nonet_df = C2W1nonet_vars, C2W1nonet_df\n",
    "            pred_df = C2pred_df\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['W2_ND1']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['W2_ND7'])\n",
    "\n",
    "        elif cohort == '1+2':\n",
    "            nonet_df = pd.concat([C1W1nonet_df, C2W1nonet_df], ignore_index=True)\n",
    "            nonet_vars = C1W1nonet_vars  # same set of columns for both cohorts\n",
    "\n",
    "            colname_map = {}\n",
    "            C2pred_keys = list(C2pred_df.columns)\n",
    "            for i, c in enumerate(list(C1pred_df.columns)):  # map column names of C2pred_df to C1pred_df (since C1W2 has different varnames)\n",
    "                colname_map[C2pred_keys[i]] = c\n",
    "\n",
    "            pred_df = pd.concat([C1pred_df, C2pred_df.rename(columns=colname_map)], ignore_index=True)\n",
    "            pred_var = zip(pred_df['ND1'], pred_df['Q68']) if drug == 'marijuana' else zip(pred_df['ND7'], pred_df['Q75'])\n",
    "\n",
    "        nonet_df = impute_MARs(nonet_vars, nonet_df)\n",
    "        discarded_vars = ['PID','PID2','AL6B','ID13','ID14_4','ID14_5','ID14_6','ID14_7','ND13','ND15_4','ND15_5','ND15_6','ND15_7',\n",
    "                    'DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d'] + [v for v in list(nonet_df.columns) if 'TEXT' in v]\n",
    "        nominal_vars = ['DM8','DM10','DM12','DM13']\n",
    "        \n",
    "        dep_var_full = []\n",
    "        for a, b in pred_var:\n",
    "            if not np.isnan(a) and not np.isnan(b):\n",
    "                y = 0 if a <= b else 1\n",
    "                # y = a - b\n",
    "                dep_var_full.append(y)\n",
    "            else:   dep_var_full.append(np.nan)\n",
    "\n",
    "        #---------------------------------- Non-network ------------------------------------------------------\n",
    "        nonet_df = pd.concat([nonet_df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        Xnonet_df = nonet_df[nonet_df['pred'].notna()].drop(discarded_vars+['pred'], axis=1)\n",
    "        X_ordinal_df = Xnonet_df.drop(nominal_vars, axis=1)\n",
    "        X_nominal_df = Xnonet_df[nominal_vars]\n",
    "\n",
    "        # Encode\n",
    "        Xenc_ordinal_df = X_ordinal_df.astype('str').apply(LabelEncoder().fit_transform)\n",
    "        Xenc_ordinal_df = Xenc_ordinal_df.where(~X_ordinal_df.isna(), X_ordinal_df)  # Do not encode the NaNs\n",
    "\n",
    "        nominal_cols =[]\n",
    "        for v in nominal_vars:\n",
    "            nominal_cols.append(pd.get_dummies(X_nominal_df[v], prefix=v))\n",
    "        Xenc_nominal_df = pd.concat(nominal_cols, axis=1)\n",
    "\n",
    "        Xenc_df = pd.concat([Xenc_ordinal_df, Xenc_nominal_df], axis=1)\n",
    "\n",
    "        # Group non-network features\n",
    "        f_nonet_dict = {}\n",
    "        fgroups_nonet = ['SC', 'DM', 'TB', 'AL', 'ID', 'ND', 'DA', 'OD', 'TX', 'AC', 'CJ']  # feature groups\n",
    "        for g in fgroups_nonet:\n",
    "            features = [c for c in Xenc_df if c.startswith(g)]\n",
    "            f_indices = [Xenc_df.columns.get_loc(c) for c in Xenc_df if c.startswith(g)]  # column indices of the group's features\n",
    "            f_nonet_dict[g] = f_indices\n",
    "\n",
    "        # Impute\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        X_imp = imp.fit_transform(Xenc_df)\n",
    "\n",
    "        # Load corrs_dict (non-network):\n",
    "        with open(f\"saved-vars/C{''.join(str(cohort).split('+'))}-{drug}-nonnet_CramersV.pkl\", 'rb') as file:\n",
    "            corrs_nonet_dict = pickle.load(file)\n",
    "        corrs_nonet_dict = {k: v for k, v in corrs_nonet_dict.items() if not np.isnan(v)}  # sort dict by value won't work properly if there is a key w/ nan value\n",
    "\n",
    "        #---------------------------------- Network ------------------------------------------------------\n",
    "        net_df = pd.concat([net_df, pd.DataFrame({'pred': dep_var_full})], axis=1)  # drop rows where prediction var is missing\n",
    "        Xnet_df = net_df[net_df['pred'].notna()].drop('pred', axis=1)\n",
    "\n",
    "        # Group network features\n",
    "        f_net_dict = {}\n",
    "        fgroups_net = ['NSX','NDX']  # feature groups\n",
    "        for g in fgroups_net:\n",
    "            features = [c for c in Xnet_df if c.startswith(g) or g[:2] in c]\n",
    "            f_indices = [Xnet_df.columns.get_loc(c) for c in Xnet_df if c.startswith(g)]  # column indices of the group's features\n",
    "            f_net_dict[g] = f_indices\n",
    "\n",
    "        X_np = Xnet_df.to_numpy()\n",
    "\n",
    "        # Load corrs_dict (network):\n",
    "        with open(f\"saved-vars/C{''.join(str(cohort).split('+'))}-{drug}-net_CramersV.pkl\", 'rb') as file:\n",
    "            corrs_net_dict = pickle.load(file)\n",
    "        corrs_net_dict = {k: v for k, v in corrs_net_dict.items() if not np.isnan(v)}  # sort dict by value won't work properly if there is a key w/ nan value\n",
    "        #-------------------------------------------------------------------------------------------\n",
    "\n",
    "        corrs_dict = corrs_nonet_dict | corrs_net_dict\n",
    "        sorted_corrs_dict = dict(sorted(corrs_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        sorted_corrs_list = [v for v in sorted_corrs_dict.keys()]\n",
    "\n",
    "        # Learning curve\n",
    "        y = np.array(dep_var_full)\n",
    "        y = y[~np.isnan(y)]\n",
    "        baseline = stats.mode(y)[1][0]/len(y)\n",
    "\n",
    "        # Static grouping\n",
    "        for gname, fsubs in fsubs_netnonnet.items():\n",
    "            X_nonet = X_imp[:, [f for fgroup in [f_nonet_dict[s] for s in fsubs if s in fgroups_nonet] for f in fgroup]]\n",
    "            X_net = X_np[:, [f for fgroup in [f_net_dict[s] for s in fsubs if s in fgroups_net] for f in fgroup]]\n",
    "            X = np.concatenate((X_nonet, X_net), axis=1)\n",
    "            savefile = f'plots/analysis/learning_curve/nonnet+net/C{cohort}-{drug}/C{cohort}-{drug}-{gname}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, gname, savefile=savefile, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(gname)\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        # Dynamic grouping\n",
    "        train_vars = []\n",
    "        for i in range(N_top):\n",
    "            var = sorted_corrs_list[i]\n",
    "            if var in nominal_vars:\n",
    "                train_vars.extend([v for v in Xenc_df if v.startswith(var)])\n",
    "            else:   train_vars.append(var)\n",
    "\n",
    "            col_indices_nonet = sorted([Xenc_df.columns.get_loc(v) for v in train_vars if v in list(Xenc_df.columns)])\n",
    "            col_indices_net = sorted([Xnet_df.columns.get_loc(v) for v in train_vars if v in list(Xnet_df.columns)])\n",
    "            X = np.concatenate((X_imp[:,col_indices_nonet], X_np[:,col_indices_net]), axis=1)\n",
    "            savefile = f'plots/analysis/learning_curve/nonnet+net/C{cohort}-{drug}/C{cohort}-{drug}-top{i+1}_learningCurve.pdf'\n",
    "            lg_score, dt_score, svm_score = plot_LC_tuned(X, y, cohort, drug, train_vars, savefile=savefile, baseline=baseline)\n",
    "            scores_dict[f'{cohort}-{drug}-fgroup'].append(sorted_corrs_list[:i+1])\n",
    "            scores_dict[f'{cohort}-{drug}-LG'].append(lg_score)\n",
    "            scores_dict[f'{cohort}-{drug}-DT'].append(dt_score)\n",
    "            scores_dict[f'{cohort}-{drug}-SVM'].append(svm_score)\n",
    "\n",
    "        scores_dict[f'{cohort}-{drug}-baseline'] = [baseline] * (len(fsubs_netnonnet)+N_top)\n",
    "\n",
    "pd.DataFrame.from_dict(scores_dict).to_csv('results/scores_nonnetwork+network.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2abc7d1412b351c343a4d86f19894027741fcfc3457eb52dff9547c08c1a4f79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
