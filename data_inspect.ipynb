{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditionals\n",
    "\n",
    "For imputation of MAR data (all labels wrt .docx code book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TB\n",
    "\n",
    "- If TB2 = 5, then, TB2_4_TEXT = -1, TB3 = 1, TB4 = 1\n",
    "- If TB3 = 1, then TB4 = 1\n",
    "- If TB5 = 1, then TB6 = -1, TB7 = 1, TB8 = 1\n",
    "- If TB7 = 1, then TB8 = 1\n",
    "- If TB9 = 1, then TB10 = -1, TB11 = 1, TB12 = 1\n",
    "- If TB11 = 1, then TB12 = 1\n",
    "\n",
    "Combine TB2 and TB2_4_TEXT into one (take TB2 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AL\n",
    "\n",
    "- If AL1 = 2, then AL1_4_TEXT = -1, AL2_1_TEXT = -1, AL3_1_TEXT = -1, AL4 = 1, AL5 = 1, AL6A = 1, AL6B = 1\n",
    "    - If AL1 = 3, then AL1_4_TEXT = -2\n",
    "- If AL2 = 2 and/or AL3 = 2, then AL2_1_TEXT = -2 and/or AL3_1_TEXT = -2\n",
    "- If AL5 = 1, then AL6A = 1, AL6B = 1\n",
    "\n",
    "Combine AL1 and AL1_4_TEXT, AL2 and AL2_1_TEXT, AL3 and AL3_1_TEXT, AL6A and AL6B (take AL1, AL2, AL3, AL6A only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID and ND\n",
    "\n",
    "- If ID1 = 1, then ID2 = -1, ID3-12 = 1, ID15-20 = -1\n",
    "- If ID3 = 1, then ID4-12 = 1, ID15-20 = -1 (wrong labels in C1W1, changed it back in terms of .docx labels beforehand)\n",
    "- If ID17 = 1, then ID18-20 = -1\n",
    "- If ND1 = 1, then ND2 = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DA\n",
    "\n",
    "Multiple choice variable: ['DA5','DA6','DA7','DA7a-d','DA8','DA8a-d'] --> Create new variable for each choice made by participants\n",
    "\n",
    "- If DA7_1 = 0, then DA7ax = 0\n",
    "- If DA7_15 = 0, then DA7bx = 0\n",
    "- If DA7_16 = 0, then DA7cx = 0\n",
    "- If DA7_17 = 0, then DA7dx = 0\n",
    "- If DA7_18 = 1, then DA7ax = 0, DA7bx = 0, DA7cx = 0, DA7dx = 0\n",
    "\n",
    "<br>\n",
    "\n",
    "- If DA8_17 = 0, then DA8ax = 0\n",
    "- If DA8_18 = 0, then DA8bx = 0\n",
    "- If DA8_19 = 0, then DA8cx = 0\n",
    "- If DA8_20 = 0, then DA8dx = 0\n",
    "- If DA8_21 = 1, then DA8ax = 0, DA8bx = 0, DA8cx = 0, DA8dx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OD, CJ, and DM\n",
    "\n",
    "- If OD1 = 1, then OD2 = 1\n",
    "- If OD6 = 2, then OD7-11 = 1\n",
    "- If OD8 = 1, then OD9 = 1\n",
    "- If OD10 = 1, then OD11 = 1\n",
    "- If CJ3 = 1, then CJ4-7 = -1\n",
    "- If DM12 != 1, then DM13 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_MARs(vars, df):\n",
    "\n",
    "    new_vars = []\n",
    "    for v in vars:\n",
    "        col = df[v]\n",
    "        if v == 'TB2':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 5:\n",
    "                    df.at[idx, 'TB2_4_TEXT'] = -1\n",
    "                    df.at[idx, 'TB3'] = 1\n",
    "                    df.at[idx, 'TB4'] = 1\n",
    "        if v == 'TB3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB4'] = 1\n",
    "        if v == 'TB5':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB6'] = -1\n",
    "                    df.at[idx, 'TB7'] = 1\n",
    "                    df.at[idx, 'TB8'] = 1\n",
    "        if v == 'TB7':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB8'] = 1\n",
    "        if v == 'TB9':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB10'] = -1\n",
    "                    df.at[idx, 'TB11'] = 1\n",
    "                    df.at[idx, 'TB12'] = 1\n",
    "        if v == 'TB11':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'TB12'] = 1\n",
    "        if v == 'AL1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL1_4_TEXT'] = -1\n",
    "                    df.at[idx, 'AL2_1_TEXT'] = -1\n",
    "                    df.at[idx, 'AL3_1_TEXT'] = -1\n",
    "                    df.at[idx, 'AL4'] = 1\n",
    "                    df.at[idx, 'AL5'] = 1\n",
    "                    df.at[idx, 'AL6A'] = 1\n",
    "                    df.at[idx, 'AL6B'] = 1\n",
    "                elif i == 3:\n",
    "                    df.at[idx, 'AL1_4_TEXT'] = -2\n",
    "        if v == 'AL2':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL2_1_TEXT'] = -2\n",
    "        if v == 'AL3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    df.at[idx, 'AL3_1_TEXT'] = -2\n",
    "        if v == 'AL5':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'AL6A'] = 1\n",
    "                    df.at[idx, 'AL6B'] = 1\n",
    "        if v == 'AL6A':\n",
    "            for idx, i in enumerate(col):\n",
    "                if pd.isnull(df.loc[idx, v]) and not pd.isnull(df.loc[idx, 'AL6B']):\n",
    "                    df.at[idx, v] = df.at[idx, 'AL6B']\n",
    "        if v == 'ID1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'ID2'] = -1\n",
    "                    for j in range(3,13):\n",
    "                        df.at[idx, f'ID{j}'] = 1\n",
    "                    for j in range(15,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ID3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(4,13):\n",
    "                        df.at[idx, f'ID{j}'] = 1\n",
    "                    for j in range(15,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ID17':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(18,21):\n",
    "                        df.at[idx, f'ID{j}'] = -1\n",
    "        if v == 'ND1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'ND2'] = -1\n",
    "        if v in ['DA5','DA6','DA7','DA7a','DA7b','DA7c','DA7d','DA8','DA8a','DA8b','DA8c','DA8d']:\n",
    "            col_nonan = col.dropna()\n",
    "            choices = sorted(set(list(itertools.chain.from_iterable([entry.split(',') for entry in col_nonan]))), key=int)  # all choices made\n",
    "            names = [f'{v}_{c}' for c in choices]\n",
    "            new_vars.extend(names)\n",
    "            for j, c in enumerate(choices):\n",
    "                newcol = []\n",
    "                for idx, i in enumerate(col):\n",
    "                    if not pd.isnull(df.loc[idx, v]):\n",
    "                        newcol.append(1) if c in i.split(',') else newcol.append(0)\n",
    "                    else:   newcol.append(np.nan)\n",
    "                df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
    "        if v == 'OD1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD2'] = 1\n",
    "        if v == 'OD6':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 2:\n",
    "                    for j in range(7,12):\n",
    "                        df.at[idx, f'OD{j}'] = 1\n",
    "        if v == 'OD8':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD9'] = 1\n",
    "        if v == 'OD10':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    df.at[idx, 'OD11'] = 1\n",
    "        if v == 'CJ3':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for j in range(4,8):\n",
    "                        df.at[idx, f'CJ{j}'] = -1\n",
    "        if v == 'DM12':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i != 1:\n",
    "                    df.at[idx, 'DM13'] = -1\n",
    "\n",
    "    for v in new_vars:\n",
    "        col = df[v]\n",
    "        if v == 'DA7_1':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA7a_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA7_15':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA7b_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA7_16':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA7c_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA7_17':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA7d_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA7_18':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith(('DA7a_','DA7b_','DA7c_','DA7d_')) and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA8_17':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA8a_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA8_18':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA8b_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA8_19':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA8c_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA8_20':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 0:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith('DA8d_') and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "        if v == 'DA8_21':\n",
    "            for idx, i in enumerate(col):\n",
    "                if i == 1:\n",
    "                    for vv in [name for name in list(df.columns) if name.startswith(('DA8a_','DA8b_','DA8c_','DA8d_')) and 'TEXT' not in name]:\n",
    "                        df.at[idx, vv] = 0\n",
    "\n",
    "\n",
    "    vars_mixed = ['TB2_4_TEXT','TB6','TB10','AL1_4_TEXT','AL2_1_TEXT','AL3_1_TEXT','ID2','ND2']\n",
    "\n",
    "    for v in vars_mixed:\n",
    "        col = df[v]\n",
    "        if v[-4:] == 'TEXT':  # e.g., modify TB2 column instead of TB2_4_TEXT\n",
    "            v = v.split('_')[0]\n",
    "        for idx, i in enumerate(col):\n",
    "            if 0 <= i <= 14:    df.at[idx, v] = 0  # children\n",
    "            elif 15 <= i <= 24: df.at[idx, v] = 1  # youth\n",
    "            elif 25 <= i <= 64: df.at[idx, v] = 2  # adult\n",
    "            elif i >= 65:       df.at[idx, v] = 3  # senior\n",
    "            elif i == -1:       df.at[idx, v] = 4  # never\n",
    "            elif i == -2:       df.at[idx, v] = 5  # don't know\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/original/'\n",
    "csv_file_nonet = datapath + 'C1W1 and C1W2 NonNetwork Data.csv'\n",
    "csv_file_net = datapath + 'C1W1 C1W2 With Network No NickInit_orig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonet = pd.read_csv(csv_file_nonet)\n",
    "df_net = pd.read_csv(csv_file_net)\n",
    "# print(df_net.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "537\n"
     ]
    }
   ],
   "source": [
    "fields_nonet = list(df_nonet.columns)\n",
    "fields_net = list(df_net.columns)\n",
    "\n",
    "print(len(fields_nonet))\n",
    "print(len(fields_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below blocks verifies that non-network data is subset of network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in fields_nonet:\n",
    "    if var not in fields_net:\n",
    "        print(f'variable {var} is exclusive to non-network data')\n",
    "    # else:\n",
    "    #     print(f'variable {var} is also included in network data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'C1W1_PID2' in column 2 of non-network data is in column 2 of network data\n",
      "Field 'C1W1_DM1' in column 11 of non-network data is in column 206 of network data\n",
      "Field 'C1W1_TB1' in column 12 of non-network data is in column 207 of network data\n",
      "Field 'C1W1_TB2' in column 13 of non-network data is in column 208 of network data\n",
      "Field 'C1W1_TB2_4_TEXT' in column 14 of non-network data is in column 209 of network data\n",
      "Field 'C1W1_DA8a' in column 101 of non-network data is in column 296 of network data\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 10, 11, 12, 13, 100]:\n",
    "    field = fields_nonet[i]\n",
    "    print(f\"Field '{field}' in column {fields_nonet.index(field)+1} of non-network data is in column {fields_net.index(field)+1} of network data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into separate waves. Note that for Aim 1A, network data for wave 2 is unavailable.\n",
    "\n",
    "The organization of columns in (with network) csv datafile is: (wave1, non-network, SC vars) --> (wave1, network) --> (wave1, non-network, the rest) --> (wave2, non-network)\n",
    "\n",
    "Categorize variables in txt files. Note that variable 'ND' is ambiguous: it stands for both 'network: drug use' (network) and 'non-injection drugs' (non-network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1W1nonet_idx, C1W1nonet_vars = [], []\n",
    "C1W2nonet_idx, C1W2nonet_vars = [], []\n",
    "C1W1net_idx, C1W1net_vars = [], []\n",
    "\n",
    "f1 = open(datapath + \"C1W1_nonnetwork.txt\", \"w\")  # wave 1, non-network\n",
    "f2 = open(datapath + \"C1W2_nonnetwork.txt\", \"w\")  # wave 2, non-network\n",
    "f3 = open(datapath + \"C1W1_network.txt\", \"w\")  # wave 1, network\n",
    "for field in fields_net:\n",
    "\n",
    "    affi = field[0:4]\n",
    "    varname = field[5:]\n",
    "    if field in fields_nonet:\n",
    "        if affi == 'C1W1':\n",
    "            f1.write(varname + '\\n')\n",
    "            if varname == 'TB1O':\n",
    "                C1W1nonet_vars.append('TB10')\n",
    "            elif varname == 'US':\n",
    "                C1W1nonet_vars.append('ND1')\n",
    "            elif varname == 'AC34A':\n",
    "                C1W1nonet_vars.append('AC3A')\n",
    "            else:\n",
    "                C1W1nonet_vars.append(varname)\n",
    "            C1W1nonet_idx.append(fields_net.index(field))\n",
    "        elif affi == 'C1W2':\n",
    "            f2.write(varname + '\\n')\n",
    "            C1W2nonet_vars.append(varname)\n",
    "            C1W2nonet_idx.append(fields_net.index(field))\n",
    "    elif field not in fields_nonet and affi == 'C1W1':\n",
    "        f3.write(varname + '\\n')\n",
    "        if varname == 'ND1':\n",
    "            C1W1net_vars.append('NSD1')\n",
    "        else:\n",
    "            C1W1net_vars.append(varname)\n",
    "        C1W1net_idx.append(fields_net.index(field))\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "\n",
    "C1W1nonet_df = df_net.iloc[:,C1W1nonet_idx]\n",
    "C1W2nonet_df = df_net.iloc[:,C1W2nonet_idx]\n",
    "C1W1net_df = df_net.iloc[:,C1W1net_idx]\n",
    "\n",
    "# Remove prefixes from column names\n",
    "C1W1nonet_df.columns = C1W1nonet_vars\n",
    "C1W2nonet_df.columns = C1W2nonet_vars\n",
    "C1W1net_df.columns = C1W1net_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with wave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-network (only include those available in csv)\n",
    "- SC1-7: Community participants live in\n",
    "- SC8-14: Social support\n",
    "- DM1-4: Age, gender, sexual orientation demographics\n",
    "- TB1-12: Tobacco use\n",
    "- AL1-7: Alcohol use\n",
    "- ID1-30: Injection drug use\n",
    "- ND1-16: Non-injection drug use\n",
    "- DA1-10: Drug accessibility and use patterns\n",
    "- OD1-11: Drug overdose\n",
    "- TX1-7: Substance use treatment\n",
    "- AC1-14: Adverse childhood experiences\n",
    "- CJ1-8: Criminal justice involvement\n",
    "- DM5-26: Long form demographics\n",
    "\n",
    "Network\n",
    "- NS: Network - ppl they can confide in\n",
    "- ND: Network - ppl they use drugs with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure consistency of data type e.g., no strings in numerical variables before imputing MARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1W1nonet_df = impute_MARs(C1W1nonet_vars, C1W1nonet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cell to NaN\n",
    "# C1W1nonet_df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "# C1W2nonet_df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "# C1W1net_df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "C1W1nonet_df.to_csv(datapath + 'C1W1_nonnetwork.csv', index=False)\n",
    "C1W2nonet_df.to_csv(datapath + 'C1W2_nonnetwork.csv', index=False)\n",
    "C1W1net_df.to_csv(datapath + 'C1W1_network.csv', index=False)\n",
    "\n",
    "# C1W1nonet_df.to_csv(datapath + 'pre-imputed/C1W1_nonnetwork_preimputed.csv', index=False)\n",
    "# C1W2nonet_df.to_csv(datapath + 'pre-imputed/C1W2_nonnetwork_preimputed.csv', index=False)\n",
    "# C1W1net_df.to_csv(datapath + 'pre-imputed/C1W1_network_preimputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/original/'\n",
    "csv_file = datapath + 'C2W1 C2W2 9_30_2022 With Network No NickInit_orig.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(df_net.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n"
     ]
    }
   ],
   "source": [
    "fields = list(df.columns)\n",
    "print(len(fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization is: C2W2 -> C2W1\n",
    "\n",
    "Again, wave 2 doesn't have AC features; but for Cohort 2, both waves have network features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2W1nonet_idx, C2W1nonet_vars = [], []\n",
    "C2W2nonet_idx, C2W2nonet_vars = [], []\n",
    "C2W1net_idx, C2W1net_vars = [], []\n",
    "C2W2net_idx, C2W2net_vars = [], []\n",
    "\n",
    "f1 = open(datapath + \"C2W1_nonnetwork.txt\", \"w\")  # wave 1, non-network\n",
    "f2 = open(datapath + \"C2W2_nonnetwork.txt\", \"w\")  # wave 2, non-network\n",
    "f3 = open(datapath + \"C2W1_network.txt\", \"w\")  # wave 1, network\n",
    "f4 = open(datapath + \"C2W2_network.txt\", \"w\")  # wave 2, network\n",
    "\n",
    "for field in fields:\n",
    "\n",
    "    affi = field[0:4]\n",
    "    varname = field[5:]\n",
    "    if affi == 'C2W1':\n",
    "        if not varname.startswith('NS') and not varname.startswith('NDX'):  # wave 1, non-network\n",
    "            f1.write(varname + '\\n')\n",
    "            if varname == 'TB1O':\n",
    "                C2W1nonet_vars.append('TB10')\n",
    "            elif varname == 'US':\n",
    "                C2W1nonet_vars.append('ND1')\n",
    "            elif varname == 'AC34A':\n",
    "                C2W1nonet_vars.append('AC3A')\n",
    "            else:\n",
    "                C2W1nonet_vars.append(varname)\n",
    "            C2W1nonet_idx.append(fields.index(field))\n",
    "        else:  # wave 1, network\n",
    "            f3.write(varname + '\\n')\n",
    "            C2W1net_vars.append(varname)\n",
    "            C2W1net_idx.append(fields.index(field))\n",
    "    elif affi == 'C2W2':\n",
    "        if not varname.startswith('NS') and not varname.startswith('NDX'):  # wave 2, non-network\n",
    "            f2.write(varname + '\\n')\n",
    "            if varname == 'TB1O':\n",
    "                C2W2nonet_vars.append('TB10')\n",
    "            elif varname == 'US':\n",
    "                C2W2nonet_vars.append('ND1')\n",
    "            elif varname == 'AC34A':\n",
    "                C2W2nonet_vars.append('AC3A')\n",
    "            else:\n",
    "                C2W2nonet_vars.append(varname)\n",
    "            C2W2nonet_idx.append(fields.index(field))\n",
    "        else:  # wave 2, network\n",
    "            f4.write(varname + '\\n')\n",
    "            C2W2net_vars.append(varname)\n",
    "            C2W2net_idx.append(fields.index(field))\n",
    "    \n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()\n",
    "\n",
    "C2W1nonet_df = df.iloc[:,C2W1nonet_idx]\n",
    "C2W2nonet_df = df.iloc[:,C2W2nonet_idx]\n",
    "C2W1net_df = df.iloc[:,C2W1net_idx]\n",
    "C2W2net_df = df.iloc[:,C2W2net_idx]\n",
    "\n",
    "# Remove prefixes from column names\n",
    "C2W1nonet_df.columns = C2W1nonet_vars\n",
    "C2W2nonet_df.columns = C2W2nonet_vars\n",
    "C2W1net_df.columns = C2W1net_vars\n",
    "C2W2net_df.columns = C2W2net_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n",
      "/tmp/ipykernel_391/1875228192.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(df.columns.get_loc(v)+j+1,names[j],newcol)\n"
     ]
    }
   ],
   "source": [
    "C2W1nonet_df = impute_MARs(C2W1nonet_vars, C2W1nonet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2W2nonet_df = impute_MARs(C2W2nonet_vars, C2W2nonet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2W1nonet_df.to_csv(datapath + 'C2W1_nonnetwork.csv', index=False)\n",
    "C2W2nonet_df.to_csv(datapath + 'C2W2_nonnetwork.csv', index=False)\n",
    "C2W1net_df.to_csv(datapath + 'C2W1_network.csv', index=False)\n",
    "C2W2net_df.to_csv(datapath + 'C2W2_network.csv', index=False)\n",
    "\n",
    "# C2W1nonet_df.to_csv(datapath + 'pre-imputed/C2W1_nonnetwork_preimputed.csv', index=False)\n",
    "# C2W2nonet_df.to_csv(datapath + 'pre-imputed/C2W2_nonnetwork_preimputed.csv', index=False)\n",
    "# C2W1net_df.to_csv(datapath + 'pre-imputed/C2W1_network_preimputed.csv', index=False)\n",
    "# C2W2net_df.to_csv(datapath + 'pre-imputed/C2W2_network_preimputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that variables from both waves are consistent (have one-to-one mapping, although the orderings are different), for both network and non-network data (if discard 5 new variables in C2W2 and 25 AC variables in C2W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2W1nonet_vars = [var for var in C2W1nonet_vars if not var.startswith('AC')]\n",
    "C2W2nonet_vars = [var for var in C2W2nonet_vars if var not in ['ND17','ND18','ND19','ND20','ND21']]\n",
    "sorted(C2W1nonet_vars) == sorted(C2W2nonet_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4, 2,1]\n",
    "[i for i,d in enumerate(a) if d==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New data for Cohort 2 (11/14): $80\\rightarrow 105$ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old: 80; new: 105. Both have the same number of columns? True, which is 719\n"
     ]
    }
   ],
   "source": [
    "datapath = 'data/original/'\n",
    "csv_file_1114 = datapath + 'C2W1 C2W2 11_14_2022.csv'\n",
    "df_1114 = pd.read_csv(csv_file_1114)\n",
    "\n",
    "fields = list(df_1114.columns)\n",
    "print(f'Old: {len(df)}; new: {len(df_1114)}. Both have the same number of columns? {len(list(df)) == len(list(df_1114))}, which is {len(fields)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the partipants that previously have been cleaned to new df for consistency and time saving\n",
    "\n",
    "pids = list(df['C2W2_PID'])\n",
    "for p in pids:\n",
    "    df_1114.loc[df_1114['C2W2_PID'] == p] = df.loc[df['C2W2_PID'] == p].values.flatten().tolist()\n",
    "\n",
    "df_1114.to_csv(csv_file_1114, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2W1nonet_idx, C2W1nonet_vars = [], []\n",
    "C2W2nonet_idx, C2W2nonet_vars = [], []\n",
    "C2W1net_idx, C2W1net_vars = [], []\n",
    "C2W2net_idx, C2W2net_vars = [], []\n",
    "\n",
    "\n",
    "for field in fields:\n",
    "\n",
    "    affi = field[0:4]\n",
    "    varname = field[5:]\n",
    "    if affi == 'C2W1':\n",
    "        if not varname.startswith('NS') and not varname.startswith('NDX'):  # wave 1, non-network\n",
    "            if varname == 'TB1O':\n",
    "                C2W1nonet_vars.append('TB10')\n",
    "            elif varname == 'US':\n",
    "                C2W1nonet_vars.append('ND1')\n",
    "            elif varname == 'AC34A':\n",
    "                C2W1nonet_vars.append('AC3A')\n",
    "            else:\n",
    "                C2W1nonet_vars.append(varname)\n",
    "            C2W1nonet_idx.append(fields.index(field))\n",
    "        else:  # wave 1, network\n",
    "            C2W1net_vars.append(varname)\n",
    "            C2W1net_idx.append(fields.index(field))\n",
    "    elif affi == 'C2W2':\n",
    "        if not varname.startswith('NS') and not varname.startswith('NDX'):  # wave 2, non-network\n",
    "            if varname == 'TB1O':\n",
    "                C2W2nonet_vars.append('TB10')\n",
    "            elif varname == 'US':\n",
    "                C2W2nonet_vars.append('ND1')\n",
    "            elif varname == 'AC34A':\n",
    "                C2W2nonet_vars.append('AC3A')\n",
    "            else:\n",
    "                C2W2nonet_vars.append(varname)\n",
    "            C2W2nonet_idx.append(fields.index(field))\n",
    "        else:  # wave 2, network\n",
    "            C2W2net_vars.append(varname)\n",
    "            C2W2net_idx.append(fields.index(field))\n",
    "\n",
    "\n",
    "C2W1nonet_df = df_1114.iloc[:,C2W1nonet_idx]\n",
    "C2W2nonet_df = df_1114.iloc[:,C2W2nonet_idx]\n",
    "C2W1net_df = df_1114.iloc[:,C2W1net_idx]\n",
    "C2W2net_df = df_1114.iloc[:,C2W2net_idx]\n",
    "\n",
    "# Remove prefixes from column names\n",
    "C2W1nonet_df.columns = C2W1nonet_vars\n",
    "C2W2nonet_df.columns = C2W2nonet_vars\n",
    "C2W1net_df.columns = C2W1net_vars\n",
    "C2W2net_df.columns = C2W2net_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2W1nonet_df.to_csv(datapath + 'pre-imputed/221114/C2W1_nonnetwork_preimputed.csv', index=False)\n",
    "C2W2nonet_df.to_csv(datapath + 'pre-imputed/221114/C2W2_nonnetwork_preimputed.csv', index=False)\n",
    "C2W1net_df.to_csv(datapath + 'pre-imputed/221114/C2W1_network_preimputed.csv', index=False)\n",
    "C2W2net_df.to_csv(datapath + 'pre-imputed/221114/C2W2_network_preimputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C2W1nonet_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import impute_MARs\n",
    "\n",
    "df_w1 = impute_MARs(C2W1nonet_vars, C2W1nonet_df)\n",
    "df_w2 = impute_MARs(C2W2nonet_vars, C2W2nonet_df)\n",
    "vars = ['TB4','TB8','TB12','AL5'] + [f'ID{i}' for i in range(4,13)] + [f'ND{i}' for i in range(1,13) if i !=2]\n",
    "df_w1 = df_w1[vars]\n",
    "df_w2 = df_w2[vars]\n",
    "\n",
    "for i, var in enumerate(vars):\n",
    "    df_w1.insert(df_w1.columns.get_loc(var)+1, f'W2_{vars[i]}', list(df_w2[vars[i]]))\n",
    "\n",
    "df_w1.to_csv(datapath + 'pre-imputed/221114/C2_nonnetwork_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml-basic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0b716fd70fe4f0ee7bf8c88fd7bc803e2049933a04f0da9760a5a86b1844c6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
